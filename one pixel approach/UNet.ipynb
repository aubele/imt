{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# how many GPUs are available for tensorflow-gpu\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for creating reproducible results\n",
    "SEED = 22\n",
    "np.random.seed = SEED\n",
    "\n",
    "# values for images\n",
    "IMG_WIDTH = 512\n",
    "IMG_HEIGHT = 512\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "# number of classes\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# necessary colours\n",
    "CLASS_BLACK = (0,0,0)\n",
    "CLASS_RED = (255,0,0)\n",
    "CLASS_GREEN = (0,255,0)\n",
    "CLASS_BLUE = (0,0,255)\n",
    "# all colours which identify a class\n",
    "PALETTE = [CLASS_BLACK, CLASS_RED, CLASS_GREEN, CLASS_BLUE]\n",
    "\n",
    "# directories that contain the results from pre-processing, should already exist\n",
    "IMAGE_DIRECTORY = \"./data/images/\"\n",
    "MASK_DIRECTORY = \"./data/masks/\"\n",
    "FILENAME_DIRECTORY = './data/filenames/'\n",
    "# directory to save the results, should be created\n",
    "RESULT_DIRECTORY = './data/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the essential directory if it does not exist\n",
    "if not os.path.exists(RESULT_DIRECTORY):\n",
    "    os.makedirs(RESULT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.load(FILENAME_DIRECTORY + 'X_train.npy')\n",
    "y_train = np.load(FILENAME_DIRECTORY + 'y_train.npy')\n",
    "\n",
    "X_test = np.load(FILENAME_DIRECTORY + 'X_test.npy')\n",
    "y_test = np.load(FILENAME_DIRECTORY + 'y_test.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding and the reverse operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rgbToOnehot(rgbImage, palette=PALETTE):\n",
    "    \"\"\"Implements One Hot Encoding (OHE) for RGB images, \n",
    "    where each colour represents a class\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rgbImage : numpy-array\n",
    "        RGB-image to which OHE should be applied\n",
    "    palette : list\n",
    "        All colours that define a class\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy-array\n",
    "        One Hot encoded image\n",
    "    \"\"\"\n",
    "        \n",
    "    # create new image\n",
    "    labelImage = np.zeros((rgbImage.shape[:2]), dtype=np.uint8)\n",
    "    # label the classes correctly\n",
    "    labelImage[(rgbImage==palette[0]).all(axis=2)] = 0\n",
    "    labelImage[(rgbImage==palette[1]).all(axis=2)] = 1\n",
    "    labelImage[(rgbImage==palette[2]).all(axis=2)] = 2\n",
    "    labelImage[(rgbImage==palette[3]).all(axis=2)] = 3\n",
    "    # transform them into binary values\n",
    "    onehotImage = tf.keras.utils.to_categorical(labelImage, 4)\n",
    "    return onehotImage\n",
    "\n",
    "def onehotToRgb(onehotImage, palette=PALETTE):\n",
    "    \"\"\"Reverts OHE and gives the image its initial\n",
    "    colours back\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    onehotImage : numpy-array\n",
    "        Image with OHE which should get its colour back\n",
    "    palette : list\n",
    "        All colours that define a class\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    numpy-array\n",
    "        RGB image\n",
    "    \"\"\"\n",
    "        \n",
    "    # returns indices of the highest elements\n",
    "    indices = np.argmax(onehotImage, axis=2)\n",
    "    # use the initial image shape plus three channels for rgb\n",
    "    rgbImage = np.zeros(onehotImage.shape[:2]+(3,))\n",
    "    for i, colours in enumerate(palette):\n",
    "        # get the colours back\n",
    "        rgbImage[indices==i] = colours\n",
    "    return np.uint8(rgbImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the One Hot Encoding and the reverse operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 4)\n"
     ]
    }
   ],
   "source": [
    "# load one mask and apply OHE\n",
    "test = np.load(MASK_DIRECTORY + y_train[0])\n",
    "test = rgbToOnehot(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "# check if all red pixels are transformed in the correct binary value\n",
    "l = []\n",
    "for x in test:\n",
    "    for y in x:\n",
    "        if np.all(y == (0.,1.,0.,0.)):\n",
    "            l.append(y)\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# revert OHE\n",
    "test = onehotToRgb(test)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267\n"
     ]
    }
   ],
   "source": [
    "# check if all red pixels are still available\n",
    "l = []\n",
    "for x in test:\n",
    "    for y in x:\n",
    "        if np.all(y == (255,0,0)):\n",
    "            l.append(y)\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class IMT_Generator(tf.keras.utils.Sequence):\n",
    "    \"\"\"A class for the custom generators of this application\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    filenames_images : list\n",
    "        List of all file names of the images\n",
    "    filenames_masks : list\n",
    "        List of all file names of the masks\n",
    "    batch_size :  int\n",
    "        The for the training specified batch size\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __getitem__(index)\n",
    "        Returns the from the U-Net model wanted images and \n",
    "        masks, while OHE gets applied on the masks\n",
    "    __len__()\n",
    "        Specifies the total number of batches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filenames_images, filenames_masks, batch_size=32):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        filenames_images : list\n",
    "            List of all file names of the images\n",
    "        filenames_masks : list\n",
    "            List of all file names of the masks\n",
    "        batch_size :  int\n",
    "            Defines the for the training specified batch size\n",
    "        \"\"\"\n",
    "            \n",
    "        self.batch_size = batch_size\n",
    "        self.filenames_images = filenames_images\n",
    "        self.filenames_masks = filenames_masks\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\" Specifies the total number of batches\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            number of batches\n",
    "        \"\"\"\n",
    "        return int(np.floor(len(self.filenames_images) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, index):        \n",
    "        \"\"\"Returns the from the U-Net model wanted images and \n",
    "        masks, while OHE gets applied on the masks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index : int\n",
    "            Index of the requested batch from the U-Net model,\n",
    "            after the processing of one batch the index is\n",
    "            raised\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            The images of the current batch\n",
    "        list\n",
    "            The masks of the current batch\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize both batches\n",
    "        batchX = self.filenames_images[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        batchY = self.filenames_masks[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        # load the images\n",
    "        returnX = np.array([np.load(IMAGE_DIRECTORY + str(fileName)) for fileName in batchX])\n",
    "        # load the masks and apply OHE\n",
    "        returnY = np.array([rgbToOnehot(np.load(MASK_DIRECTORY + str(fileName))) for fileName in batchY])\n",
    "        \n",
    "        return returnX, returnY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of focal loss function, Dice coefficient and intersection over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    \"\"\"Calculates Dice coefficient as a evaluation\n",
    "    score for the U-Net model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy-arrays\n",
    "        Groundtruth of the current image\n",
    "    y_pred : numpy-arrays\n",
    "        Prediction result of the current image\n",
    "    smooth : float\n",
    "        For smoothing the result\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Dice coefficient\n",
    "    \"\"\"\n",
    "    \n",
    "    # flat the arrays\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    # area of overlap\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    # calculate and return dice\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) + smooth)\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1.0):\n",
    "    \"\"\"Calculates intersection over union as a evaluation\n",
    "    score for the U-Net model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy-arrays\n",
    "        Groundtruth of the current image\n",
    "    y_pred : numpy-arrays\n",
    "        Prediction result of the current image\n",
    "    smooth : float\n",
    "        For smoothing the result\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Intersection over Union\n",
    "    \"\"\"\n",
    "        \n",
    "    # area of overlap\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])\n",
    "    # area of union\n",
    "    union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection\n",
    "    # calculate intersection over union\n",
    "    iou = K.mean((intersection + smooth) / (union + smooth), axis=0)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def categorical_focal_loss(alpha, gamma=2.):\n",
    "    \"\"\"Implementation of the focal loss function as a custom loss \n",
    "    function for keras, this function was used from a GitHub project \n",
    "    from Umberto Griffo: \n",
    "    https://github.com/umbertogriffo/focal-loss-keras\n",
    "    link acessed at: 15.12.2020 \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : numpy-arrays\n",
    "        Alpha is used to specify the weights of different categories/labels, \n",
    "        the size of the array needs to be consistent with the number of \n",
    "        classes\n",
    "    gamma : float\n",
    "        Focusing parameter for modulating factor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the calculated focal loss function for given inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights\n",
    "    alpha = np.array(alpha, dtype=np.float32)\n",
    "\n",
    "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : numpy-arrays\n",
    "            Groundtruth of the current image\n",
    "        y_pred : numpy-arrays\n",
    "            Prediction result of the current image\n",
    "        \"\"\"\n",
    "        # clip the prediction value to prevent NaN's and Inf's\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        # calculate cross entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        # calculate focal loss\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        # compute mean loss in mini_batch\n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "\n",
    "    return categorical_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createUNetModel(nFilters=32, bias_neuron=True):\n",
    "    \"\"\"Create the U-Net model after the specified architecture \n",
    "    with a few improvements, the link below explains the \n",
    "    architecture in detail\n",
    "    https://arxiv.org/pdf/1505.04597.pdf\n",
    "    link acessed at: 15.12.2020 \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nFilters : int\n",
    "        Used amount of filters for the first layer\n",
    "    bias_neuron : bool\n",
    "        Are bias neurons used across the model or not\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Created U-Net model\n",
    "    \"\"\"\n",
    "    # input layer\n",
    "    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS))\n",
    "    \n",
    "    # normalize the values\n",
    "    n = tf.keras.layers.Lambda(lambda x: x/255)(inputs)\n",
    "    \n",
    "    # --- down-sampling path --- \n",
    "    # first part\n",
    "    c1 = tf.keras.layers.Conv2D(nFilters, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(n)\n",
    "    c1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
    "    c1 = tf.keras.layers.Conv2D(nFilters, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c1)\n",
    "    c1 = tf.keras.layers.BatchNormalization()(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D((2,2), data_format='channels_last')(c1)\n",
    "    \n",
    "    # second part\n",
    "    c2 = tf.keras.layers.Conv2D(nFilters*2, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.BatchNormalization()(c2)\n",
    "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
    "    c2 = tf.keras.layers.Conv2D(nFilters*2, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c2)\n",
    "    c2 = tf.keras.layers.BatchNormalization()(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D((2,2), data_format='channels_last')(c2)\n",
    "    \n",
    "    # third part\n",
    "    c3 = tf.keras.layers.Conv2D(nFilters*4, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.BatchNormalization()(c3)\n",
    "    c3 = tf.keras.layers.Dropout(0.1)(c3)\n",
    "    c3 = tf.keras.layers.Conv2D(nFilters*4, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c3)\n",
    "    c3 = tf.keras.layers.BatchNormalization()(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D((2,2), data_format='channels_last')(c3)\n",
    "    \n",
    "    # fourth part\n",
    "    c4 = tf.keras.layers.Conv2D(nFilters*8, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = tf.keras.layers.BatchNormalization()(c4)\n",
    "    c4 = tf.keras.layers.Dropout(0.1)(c4)\n",
    "    c4 = tf.keras.layers.Conv2D(nFilters*8, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c4)\n",
    "    c4 = tf.keras.layers.BatchNormalization()(c4)\n",
    "    p4 = tf.keras.layers.MaxPooling2D((2,2), data_format='channels_last')(c4)\n",
    "    \n",
    "    # fifth part\n",
    "    c5 = tf.keras.layers.Conv2D(nFilters*16, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = tf.keras.layers.BatchNormalization()(c5)\n",
    "    c5 = tf.keras.layers.Dropout(0.1)(c5)\n",
    "    c5 = tf.keras.layers.Conv2D(nFilters*16, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c5)\n",
    "    c5 = tf.keras.layers.BatchNormalization()(c5)\n",
    "    \n",
    "    # --- up-sampling path --- \n",
    "    # first part\n",
    "    u6 = tf.keras.layers.Conv2DTranspose(nFilters*8, (2,2), strides=(2,2), use_bias=bias_neuron, padding='same')(c5)\n",
    "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
    "    c6 = tf.keras.layers.Conv2D(nFilters*8, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = tf.keras.layers.BatchNormalization()(c6)\n",
    "    c6 = tf.keras.layers.Dropout(0.1)(c6)\n",
    "    c6 = tf.keras.layers.Conv2D(nFilters*8, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c6)\n",
    "    c6 = tf.keras.layers.BatchNormalization()(c6)\n",
    "    \n",
    "    # second part\n",
    "    u7 = tf.keras.layers.Conv2DTranspose(nFilters*4, (2,2), strides=(2,2), use_bias=bias_neuron, padding='same')(c6)\n",
    "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
    "    c7 = tf.keras.layers.Conv2D(nFilters*4, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = tf.keras.layers.BatchNormalization()(c7)\n",
    "    c7 = tf.keras.layers.Dropout(0.1)(c7)\n",
    "    c7 = tf.keras.layers.Conv2D(nFilters*4, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c7)\n",
    "    c7 = tf.keras.layers.BatchNormalization()(c7)\n",
    "    \n",
    "    # third part\n",
    "    u8 = tf.keras.layers.Conv2DTranspose(nFilters*2, (2,2), strides=(2,2), use_bias=bias_neuron, padding='same')(c7)\n",
    "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
    "    c8 = tf.keras.layers.Conv2D(nFilters*2, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = tf.keras.layers.BatchNormalization()(c8)\n",
    "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
    "    c8 = tf.keras.layers.Conv2D(nFilters*2, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c8)\n",
    "    c8 = tf.keras.layers.BatchNormalization()(c8)\n",
    "    \n",
    "    # fourth part\n",
    "    u9 = tf.keras.layers.Conv2DTranspose(nFilters, (2,2), strides=(2,2), use_bias=bias_neuron, padding='same')(c8)\n",
    "    u9 = tf.keras.layers.concatenate([u9, c1])\n",
    "    c9 = tf.keras.layers.Conv2D(nFilters, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
    "    c9 = tf.keras.layers.Conv2D(nFilters, (3,3), activation='relu', use_bias=bias_neuron, \n",
    "                                kernel_initializer='he_normal', padding='same')(c9)\n",
    "    c9 = tf.keras.layers.BatchNormalization()(c9)\n",
    "    \n",
    "    # output layer\n",
    "    outputs = tf.keras.layers.Conv2D(NUM_CLASSES, (1,1), activation='softmax')(c9)\n",
    "    \n",
    "    # create and compile the model\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss=[categorical_focal_loss(alpha=[[.1, .35, .35, .35]], gamma=0.1)], \n",
    "                  metrics=[dice_coef, iou_coef, 'accuracy'])\n",
    "    # print summary\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512, 512, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 512, 512, 16) 448         lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 512, 512, 16) 64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 512, 512, 16) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 512, 512, 16) 2320        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 512, 512, 16) 64          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 256, 256, 16) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 256, 32) 4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 256, 256, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256, 256, 32) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 256, 256, 32) 9248        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 256, 256, 32) 128         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 32) 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 128, 64) 256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 128, 64) 36928       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128, 128, 64) 256         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 64, 64, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 64, 64, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, 64, 128)  147584      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 64, 64, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 32, 32, 256)  1024        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 32, 32, 256)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 256)  590080      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 32, 32, 256)  1024        conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 128)  131200      batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 64, 64, 256)  0           conv2d_transpose_4[0][0]         \n",
      "                                                                 batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, 64, 128)  512         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 64, 64, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  147584      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 64, 64, 128)  512         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 128, 128, 64) 32832       batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 128, 128 0           conv2d_transpose_5[0][0]         \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128, 128, 64) 256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128, 128, 64) 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 64) 36928       dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 128, 128, 64) 256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 256, 256, 32) 8224        batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 256, 256, 64) 0           conv2d_transpose_6[0][0]         \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 256, 256, 32) 128         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 256, 256, 32) 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 32) 9248        dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 256, 256, 32) 128         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 512, 512, 16) 2064        batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 512, 512, 32) 0           conv2d_transpose_7[0][0]         \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 512, 512, 16) 4624        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 512, 512, 16) 64          conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 512, 512, 16) 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 512, 512, 16) 2320        dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 512, 512, 16) 64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 512, 512, 4)  68          batch_normalization_35[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 1,947,044\n",
      "Trainable params: 1,944,100\n",
      "Non-trainable params: 2,944\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = createUNetModel(32, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the generators\n",
    "batchSize = 6\n",
    "\n",
    "train_generator = IMT_Generator(X_train, y_train, batchSize)\n",
    "test_generator = IMT_Generator(X_test, y_test, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create callbacks\n",
    "\n",
    "# checkpoints so the model is saved\n",
    "modelCheckpoint = tf.keras.callbacks.ModelCheckpoint('unet_imt_1px.h5', mode='max', verbose=1, monitor='val_iou_coef', save_best_only=True)\n",
    "# early stopping against overfitting\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(mode='max', monitor='val_iou_coef', patience=10, verbose=1)\n",
    "# tensorboard for graphics and statistics\n",
    "tensorBoard = tf.keras.callbacks.TensorBoard(log_dir='logs', write_graph=True)\n",
    "\n",
    "callbacks = [modelCheckpoint, earlyStopping, tensorBoard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\unet_py37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "424/425 [============================>.] - ETA: 0s - loss: 0.0439 - dice_coef: 0.8402 - iou_coef: 0.5863 - acc: 0.9466\n",
      "Epoch 00001: val_iou_coef improved from -inf to 0.92301, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 187s 439ms/step - loss: 0.0438 - dice_coef: 0.8406 - iou_coef: 0.5871 - acc: 0.9467 - val_loss: 0.0053 - val_dice_coef: 0.9966 - val_iou_coef: 0.9230 - val_acc: 0.9970\n",
      "Epoch 2/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 0.0024 - dice_coef: 0.9977 - iou_coef: 0.9562 - acc: 0.9973\n",
      "Epoch 00002: val_iou_coef improved from 0.92301 to 0.97469, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 172s 405ms/step - loss: 0.0024 - dice_coef: 0.9977 - iou_coef: 0.9563 - acc: 0.9973 - val_loss: 0.0016 - val_dice_coef: 0.9982 - val_iou_coef: 0.9747 - val_acc: 0.9976\n",
      "Epoch 3/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 0.0013 - dice_coef: 0.9982 - iou_coef: 0.9811 - acc: 0.9977\n",
      "Epoch 00003: val_iou_coef improved from 0.97469 to 0.98624, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 173s 407ms/step - loss: 0.0013 - dice_coef: 0.9982 - iou_coef: 0.9812 - acc: 0.9977 - val_loss: 0.0011 - val_dice_coef: 0.9984 - val_iou_coef: 0.9862 - val_acc: 0.9978\n",
      "Epoch 4/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 9.6390e-04 - dice_coef: 0.9985 - iou_coef: 0.9878 - acc: 0.9979\n",
      "Epoch 00004: val_iou_coef improved from 0.98624 to 0.99021, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 180s 424ms/step - loss: 9.6339e-04 - dice_coef: 0.9985 - iou_coef: 0.9878 - acc: 0.9979 - val_loss: 0.0010 - val_dice_coef: 0.9984 - val_iou_coef: 0.9902 - val_acc: 0.9979\n",
      "Epoch 5/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 8.1731e-04 - dice_coef: 0.9986 - iou_coef: 0.9908 - acc: 0.9981\n",
      "Epoch 00005: val_iou_coef improved from 0.99021 to 0.99141, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 172s 405ms/step - loss: 8.1738e-04 - dice_coef: 0.9986 - iou_coef: 0.9908 - acc: 0.9981 - val_loss: 0.0011 - val_dice_coef: 0.9984 - val_iou_coef: 0.9914 - val_acc: 0.9978\n",
      "Epoch 6/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 7.3065e-04 - dice_coef: 0.9987 - iou_coef: 0.9924 - acc: 0.9982\n",
      "Epoch 00006: val_iou_coef improved from 0.99141 to 0.99287, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 179s 420ms/step - loss: 7.3044e-04 - dice_coef: 0.9987 - iou_coef: 0.9924 - acc: 0.9982 - val_loss: 8.5177e-04 - val_dice_coef: 0.9986 - val_iou_coef: 0.9929 - val_acc: 0.9981\n",
      "Epoch 7/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 7.0670e-04 - dice_coef: 0.9987 - iou_coef: 0.9932 - acc: 0.9982\n",
      "Epoch 00007: val_iou_coef improved from 0.99287 to 0.99296, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 178s 419ms/step - loss: 7.0638e-04 - dice_coef: 0.9987 - iou_coef: 0.9932 - acc: 0.9982 - val_loss: 0.0010 - val_dice_coef: 0.9985 - val_iou_coef: 0.9930 - val_acc: 0.9979\n",
      "Epoch 8/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 6.5101e-04 - dice_coef: 0.9988 - iou_coef: 0.9939 - acc: 0.9983\n",
      "Epoch 00008: val_iou_coef improved from 0.99296 to 0.99361, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 168s 396ms/step - loss: 6.5078e-04 - dice_coef: 0.9988 - iou_coef: 0.9939 - acc: 0.9983 - val_loss: 7.8366e-04 - val_dice_coef: 0.9986 - val_iou_coef: 0.9936 - val_acc: 0.9981\n",
      "Epoch 9/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 6.1990e-04 - dice_coef: 0.9988 - iou_coef: 0.9945 - acc: 0.9983\n",
      "Epoch 00009: val_iou_coef improved from 0.99361 to 0.99381, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 167s 394ms/step - loss: 6.1976e-04 - dice_coef: 0.9988 - iou_coef: 0.9945 - acc: 0.9983 - val_loss: 9.3846e-04 - val_dice_coef: 0.9986 - val_iou_coef: 0.9938 - val_acc: 0.9980\n",
      "Epoch 10/10\n",
      "424/425 [============================>.] - ETA: 0s - loss: 6.1357e-04 - dice_coef: 0.9988 - iou_coef: 0.9946 - acc: 0.9983\n",
      "Epoch 00010: val_iou_coef improved from 0.99381 to 0.99461, saving model to unet_imt_1px.h5\n",
      "425/425 [==============================] - 167s 394ms/step - loss: 6.1310e-04 - dice_coef: 0.9988 - iou_coef: 0.9946 - acc: 0.9983 - val_loss: 8.1570e-04 - val_dice_coef: 0.9988 - val_iou_coef: 0.9946 - val_acc: 0.9983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169d8488>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final call for training\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=test_generator,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\unet_py37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\unet_py37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\anaconda3\\envs\\unet_py37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# load the best performing model\n",
    "model = tf.keras.models.load_model('./unet_imt_1px.h5', compile=False)\n",
    "# compile it separately else problems occur with the custom loss function / custom evaluation score\n",
    "model.compile(optimizer='adam', \n",
    "              loss=[categorical_focal_loss(alpha=[[.1, .35, .35, .35]], gamma=0.1)], \n",
    "              metrics=[dice_coef, iou_coef, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 11s 103ms/step - loss: 8.1570e-04 - dice_coef: 0.9988 - iou_coef: 0.9946 - acc: 0.9983\n"
     ]
    }
   ],
   "source": [
    "# calculate and save the achieved scores\n",
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "np.save(RESULT_DIRECTORY + 'scores.npy', scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107/107 [==============================] - 11s 102ms/step\n"
     ]
    }
   ],
   "source": [
    "# create and save the results\n",
    "results = model.predict_generator(test_generator, verbose=1)\n",
    "# the file 'results.npy' is really big (multiple GBs) since it contains float values\n",
    "np.save(RESULT_DIRECTORY + 'results.npy', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 604.85,
   "position": {
    "height": "561.85px",
    "left": "1597px",
    "right": "20px",
    "top": "103px",
    "width": "339px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
